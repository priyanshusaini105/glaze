/**
 * Isolated LLM Provider
 * 
 * Handles all LLM interactions for enrichment fallback
 * Supports multiple models (Claude, GPT, etc.)
 * Manages context, streaming, and error handling
 */

import { generateObject, generateText } from "ai";

export interface LLMConfig {
  provider: "anthropic" | "openai";
  model: string;
  apiKey: string;
  temperature?: number;
  maxTokens?: number;
}

export interface LLMFallbackRequest {
  field: string;
  context: Record<string, string>;
  examples?: string[];
}

export interface LLMFallbackResponse {
  value: string;
  confidence: number;
  reasoning: string;
}

export class LLMProvider {
  private config: LLMConfig;

  constructor(config: LLMConfig) {
    this.config = config;
  }

  /**
   * Use LLM to infer missing enrichment field
   */
  async inferField(request: LLMFallbackRequest): Promise<LLMFallbackResponse> {
    const prompt = this.buildPrompt(request);

    const response = await generateText({
      model: this.config.model,
      prompt,
      temperature: this.config.temperature ?? 0.3,
      maxTokens: this.config.maxTokens ?? 256,
    });

    return {
      value: response.text,
      confidence: 0.6, // LLM inferences have lower confidence
      reasoning: "Generated by LLM fallback",
    };
  }

  /**
   * Use LLM to validate and correct enrichment data
   */
  async validateData(
    field: string,
    value: string,
    context: Record<string, string>
  ): Promise<{
    valid: boolean;
    corrected?: string;
    reasoning: string;
  }> {
    const prompt = `
      Field: ${field}
      Value: ${value}
      Context: ${JSON.stringify(context, null, 2)}
      
      Is this value valid for the field? If not, provide a corrected value.
      Respond with JSON: { "valid": boolean, "corrected": string | null, "reasoning": string }
    `;

    const response = await generateText({
      model: this.config.model,
      prompt,
      temperature: 0.1,
      maxTokens: 256,
    });

    try {
      return JSON.parse(response.text);
    } catch {
      return {
        valid: true,
        reasoning: "Could not parse LLM response",
      };
    }
  }

  private buildPrompt(request: LLMFallbackRequest): string {
    let prompt = `You are a data enrichment expert. Based on the provided context, infer the value for the following field:\n\n`;
    prompt += `Field: ${request.field}\n`;
    prompt += `Context:\n${Object.entries(request.context)
      .map(([k, v]) => `  ${k}: ${v}`)
      .join("\n")}\n\n`;

    if (request.examples?.length) {
      prompt += `Examples:\n${request.examples.map((e) => `  - ${e}`).join("\n")}\n\n`;
    }

    prompt += `Provide only the inferred value, nothing else.`;

    return prompt;
  }
}

export const createLLMProvider = (config: LLMConfig): LLMProvider => {
  return new LLMProvider(config);
};
